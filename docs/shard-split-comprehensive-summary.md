# 分片分裂综合解决方案

## 1. 核心设计理念与原则

### 1.1 总体方案
采用 **"快照 + 增量日志 + 缓存切换"** 的零停机分裂方案，确保数据一致性和高可用性。

### 1.2 设计原则
- **零停机**：分裂过程中服务持续可用
- **数据一致性**：保证数据不丢失、不重复
- **可靠性**：支持从任意状态回滚，处理各种异常场景
- **性能优化**：减少对源分片的影响，提高分裂效率
- **可观测性**：详细的状态监控和进度跟踪

## 2. 优化后的分裂流程

### 2.1 六阶段完整流程

```
┌─────────────────────────────────────────────────────────────────┐
│                     分裂前状态                                   │
│  ┌─────────────────────────────────────┐                        │
│  │        shard_0001                   │                        │
│  │    slots: [0, 8192)                 │                        │
│  │    数据量: 10GB                     │                        │
│  │    状态: Normal                     │                        │
│  └─────────────────────────────────────┘                        │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ trigger_split(split_slot=4096)
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                   阶段 1: 准备阶段 (Preparing)                    │
│  - 验证源分片状态 (Normal, 未分裂)                              │
│  - 验证分裂点有效性 (split_slot ∈ (start, end))                │
│  - 验证目标分片存在且健康                                        │
│  - 创建 SplitTask                                                │
│  - 更新源/目标分片状态为 Splitting                              │
│  - 在路由表中添加分裂信息                                        │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ target_ready
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│             阶段 2: 快照传输 (SnapshotTransfer)                  │
│  源分片:                                                          │
│    - 获取并保护最新 Raft 快照                                    │
│    - 筛选 slot ∈ [4096, 8192) 的数据                            │
│    - 分块压缩传输到目标分片                                    │
│                                                                  │
│  目标分片:                                                        │
│    - 接收快照数据并验证完整性                                    │
│    - 加载快照到状态机                                            │
│    - 设置 applied_index = snapshot_index                        │
│    - 通过 Raft InstallSnapshot 同步到所有节点                    │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ snapshot_done
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│           阶段 3: 增量追赶 (CatchingUp)                          │
│  源分片:                                                          │
│    - 筛选 slot ∈ [4096, 8192) 的写操作日志                      │
│    - 按 Raft Index 顺序转发到目标分片                            │
│    - 定期上报进度                                                │
│                                                                  │
│  目标分片:                                                        │
│    - 接收日志条目                                                │
│    - 按顺序 apply 到状态机                                       │
│    - 更新 applied_index                                          │
│                                                                  │
│  追赶指标:                                                        │
│    delay = source.last_index - target.applied_index              │
│    当 delay < catch_up_threshold (100) 时进入下一阶段           │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ delay < threshold
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│             阶段 4: 请求缓冲 (Buffering)                         │
│  源分片:                                                          │
│    - 对 slot ≥ split_slot 的写请求进入缓存队列                  │
│    - 不处理请求，只缓存（避免数据覆盖）                          │
│                                                                  │
│  目标分片:                                                        │
│    - 继续接收增量日志                                            │
│    - 直到 applied_index == source.last_index                    │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ caught_up
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│             阶段 5: 路由切换 (Switching)                          │
│  Pilot:                                                           │
│    - 发送 SwitchPrepare 到所有节点                                │
│    - 收集确认，超过半数后进入提交阶段                            │
│    - 发送 SwitchCommit 到所有节点                                │
│    - 原子更新路由表版本                                          │
│    - 更新槽位映射:                                               │
│      * shard_0001: [0, 4096)                                    │
│      * shard_0002: [4096, 8192)                                 │
│                                                                  │
│  源分片:                                                          │
│    - 对缓存的请求批量返回 MOVED                                  │
│    - 格式: MOVED <slot> <target_addr>                           │
│                                                                  │
│  客户端:                                                          │
│    - 收到 MOVED 后重定向到目标分片                               │
│    - 重新执行请求                                                │
└─────────────────────────────────────────────────────────────────┘
                            │
                            │ switched
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│             阶段 6: 清理 (Cleanup)                               │
│  源分片:                                                          │
│    - 删除 slot ∈ [4096, 8192) 的数据（可延迟执行）              │
│    - 清除分裂状态                                                │
│    - 状态恢复为 Normal                                           │
│                                                                  │
│  Pilot:                                                           │
│    - 更新 SplitTask 状态为 Completed                             │
│    - 从路由表移除分裂信息                                        │
│    - 更新分片元数据                                              │
└─────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                     分裂后状态                                   │
│  ┌─────────────────┐  ┌─────────────────┐                      │
│  │  shard_0001     │  │  shard_0002     │                      │
│  │ slots:[0,4096)  │  │ slots:[4096,8192)│                     │
│  │ 数据量: ~5GB    │  │ 数据量: ~5GB    │                      │
│  │ 状态: Normal    │  │ 状态: Normal    │                      │
│  └─────────────────┘  └─────────────────┘                      │
└─────────────────────────────────────────────────────────────────┘
```

## 3. 细化的状态机设计

### 3.1 11状态详细定义

| 状态 | 子状态 | 说明 |
|------|--------|------|
| Preparing | - | 准备分裂，创建目标 Shard |
| SnapshotTransfer | SnapshotGenerating | 源 Shard 生成快照中 |
|  | SnapshotTransmitting | 快照传输中 |
|  | SnapshotLoading | 目标 Shard 加载快照中 |
| CatchingUp | NormalCatching | 正常追赶增量日志 |
|  | SlowCatching | 追赶缓慢，触发优化措施 |
| Buffering | BufferStarting | 开始缓存请求 |
|  | BufferRunning | 缓存请求中 |
|  | BufferFlushing | 刷新缓存请求 |
| Switching | SwitchPreparing | 准备切换路由 |
|  | SwitchCommitting | 提交路由切换 |
|  | SwitchCompleted | 路由切换完成 |
| Cleanup | DataCleaning | 清理源 Shard 数据 |
|  | MetaUpdating | 更新元数据 |
| Paused | - | 分裂暂停 |
| Cancelling | - | 正在取消分裂 |
| RolledBack | - | 分裂已回滚 |

### 3.2 状态转换图

```
┌──────────┐     ┌──────────┐     ┌──────────┐
│  Normal  │────▶│Preparing │────▶│Snapshot  │
└──────────┘     └────┬─────┘     └────┬─────┘
                      │                │
                      ▼                ▼
┌──────────┐     ┌──────────┐     ┌──────────┐
│Cancelling│◀────│  Paused  │◀────│CatchingUp│
└──────────┘     └────┬─────┘     └────┬─────┘
                      │                │
                      ▼                ▼
┌──────────┐     ┌──────────┐     ┌──────────┐
│RolledBack│◀────│Switching │◀────│Buffering │
└──────────┘     └────┬─────┘     └──────────┘
                      │
                      ▼
┌──────────┐     ┌──────────┐
│  Normal  │◀────│ Cleanup  │
└──────────┘     └──────────┘
```

## 4. 关键技术实现

### 4.1 快照传输优化

#### 4.1.1 增量快照设计
- **设计**：只传输自上次快照以来的变更数据
- **实现**：
  ```rust
  struct IncrementalSnapshot {
      base_index: u64,           // 基础快照索引
      current_index: u64,        // 当前快照索引
      slot_range: (u32, u32),    // 槽位范围
      delta_data: Vec<Entry>,    // 增量数据
  }
  ```
- **优势**：大幅减少快照数据量，降低对源分片的影响

#### 4.1.2 并行快照生成与传输
- **源分片**：多线程并行生成快照
- **网络层**：支持多连接并行传输
- **目标分片**：并行验证和加载快照数据
- **优势**：提高快照传输速度，减少分裂总耗时

#### 4.1.3 快照保护机制
- **防止快照被删除**：
  ```rust
  struct SnapshotManager {
      snapshots: Vec<Snapshot>,           // 所有快照
      protected_snapshots: HashSet<u64>,  // 受保护的快照索引集合
  }
  ```
- **自动管理**：分裂任务完成或取消时自动解除保护

### 4.2 增量日志转发机制

#### 4.2.1 可靠日志转发
- **基于 Raft 协议**：源分片作为目标分片的 "伪领导者"
- **使用 AppendEntries RPC**：保证日志转发的可靠性
- **断点续传**：记录已转发的日志索引，支持恢复

#### 4.2.2 分层日志筛选
- **Raft 层**：根据 slot 范围初步筛选日志
- **应用层**：精确筛选需要转发的键值对操作
- **优势**：提高日志筛选效率，减少不必要的转发

### 4.3 双阶段路由切换

#### 4.3.1 准备阶段 (SwitchPrepare)
1. Pilot 发送 SwitchPrepare 到所有节点
2. 节点准备新路由表，返回确认
3. Pilot 收集确认，超过半数后进入提交阶段

#### 4.3.2 提交阶段 (SwitchCommit)
4. Pilot 发送 SwitchCommit 到所有节点
5. 节点原子切换路由表
6. 节点返回切换完成确认

#### 4.3.3 版本化路由表
- **设计**：为每个路由表分配唯一版本号
- **优势**：避免路由表回滚和版本冲突

### 4.4 请求缓冲机制

#### 4.4.1 缓冲设计
```rust
struct BufferedRequest {
    command: Command,
    slot: u32,
    timestamp: Instant,
}
```

#### 4.4.2 关键参数
- **buffer_timeout**：5秒，缓存请求最大等待时间
- **buffer_max_size**：10000，最大缓存请求数
- **优势**：防止在路由切换前数据覆盖

## 5. 数据一致性保证

### 5.1 快照一致性
- **原子快照生成**：源分片使用读写锁保证数据一致性
- **快照与日志关联验证**：目标分片验证快照索引与后续日志的连续性

### 5.2 增量日志一致性
- **有序日志转发**：严格按照 Raft Index 顺序转发
- **日志重复检测**：目标分片对接收的日志进行重复检测

### 5.3 路由切换一致性
- **基于 Raft 的路由表存储**：确保所有 Pilot 节点持有一致的路由表
- **双阶段切换**：确保所有节点原子性切换路由

## 6. 异常处理机制

### 6.1 全面的异常场景覆盖

#### 6.1.1 源分片崩溃处理
- 自动切换到备用源分片继续分裂
- 基于 Raft 日志恢复分裂状态

#### 6.1.2 Pilot 崩溃处理
- Pilot 集群使用 Raft 协议保证高可用
- 分裂任务状态存储在 Raft 日志中，新 leader 可以继续管理

#### 6.1.3 网络分区处理
- 实现网络分区检测和自动恢复机制
- 网络分区时暂停分裂，恢复后继续

### 6.2 可靠的回滚机制

#### 6.2.1 事务性状态管理
```rust
type SplitTransaction struct {
    task_id: string,
    operations: Vec<SplitOperation>, // 待执行的操作
    state: TransactionState,
}
```

#### 6.2.2 分层回滚策略
根据分裂阶段采用不同的回滚策略：
- **SnapshotTransfer**：简单清理目标 Shard 数据
- **CatchingUp**：清理目标 Shard 数据和日志
- **Buffering**：恢复源 Shard 正常处理，清理目标 Shard
- **Switching**：回滚路由表，恢复源 Shard

## 7. 性能优化策略

### 7.1 源分片负载保护

#### 7.1.1 限流机制
```rust
type SplitLimiter struct {
    snapshot_rate: RateLimiter, // 快照生成速率限制
    log_forward_rate: RateLimiter, // 日志转发速率限制
}
```

#### 7.1.2 后台低优先级执行
- 分裂相关操作在后台以低优先级执行
- 减少对前台服务的影响

### 7.2 大规模集群优化

#### 7.2.1 分层 Pilot 架构
- **Global Pilot**：管理全局路由表和分裂策略
- **Regional Pilot**：管理区域内的分裂任务
- **优势**：提高大规模集群下的分裂任务管理效率

#### 7.2.2 分裂任务调度算法
- 智能分裂任务调度，避免同时执行过多分裂任务
- 检查集群负载和源分片负载，均衡系统资源

### 7.3 客户端处理优化

#### 7.3.1 智能重试机制
- 客户端实现指数退避重试机制，避免重试风暴

#### 7.3.2 客户端路由缓存更新
- 客户端定期主动更新路由缓存，减少 MOVED 响应

## 8. 关键数据结构

### 8.1 SplitTask
```rust
pub struct SplitTask {
    pub id: String,
    pub source_shard: ShardId,
    pub target_shard: ShardId,
    pub split_slot: u32,
    pub status: SplitStatus,
    pub progress: SplitProgress,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}
```

### 8.2 SplitSnapshot
```rust
struct SplitSnapshot {
    // 基础快照元数据
    raft_index: u64,       // 对应 Raft 日志索引
    raft_term: u64,        // 对应 Raft 任期
    
    // 分裂相关元数据
    source_shard_id: ShardId,  // 源分片 ID
    target_shard_id: ShardId,  // 目标分片 ID
    slot_range: (u32, u32),    // 包含的 slot 范围
    
    // 数据部分
    data: Vec<ShardEntry>,     // 仅包含目标 slot 范围的数据
    
    // 校验信息
    checksum: u32,             // CRC32 校验和
    timestamp: u64,            // 生成时间戳
}
```

### 8.3 IncrementalSnapshot
```rust
struct IncrementalSnapshot {
    base_index: u64,           // 基础快照索引
    current_index: u64,        // 当前快照索引
    slot_range: (u32, u32),    // 槽位范围
    delta_data: Vec<Entry>,    // 增量数据
}
```

## 9. 实现优先级

### 9.1 高优先级
- [x] SplitTask 数据结构和状态机
- [x] SplitManager 分裂协调器
- [x] HTTP API（触发分裂、查询进度、取消分裂）
- [x] Node 端路由表分裂状态同步
- [x] MOVED/TRYAGAIN 响应处理
- [ ] 快照传输（筛选槽位范围）
- [ ] 增量日志转发
- [ ] 请求缓冲队列

### 9.2 中优先级
- [ ] 分裂进度上报
- [ ] 自动触发分裂
- [ ] 分裂调度优化

### 9.3 低优先级
- [ ] 分裂回滚测试
- [ ] 端到端集成测试
- [ ] 监控和告警
- [ ] 分片合并（Merge）

## 10. 总结

本综合解决方案整合了原始设计、设计评审、优化设计、流程分析、快照设计和快照传输设计的所有关键技术点，形成了一个完整、可靠、高性能的分片分裂方案。该方案通过细化状态机、优化快照传输、实现双阶段路由切换、完善异常处理机制等措施，解决了原始设计中的性能瓶颈、数据一致性和可靠性问题，能够满足大规模分布式系统的分片分裂需求。

该方案的核心优势在于：
- **零停机**：分裂过程中服务持续可用
- **高可靠性**：支持从任意状态回滚，处理各种异常场景
- **高性能**：通过增量快照、并行处理等优化，减少对系统的影响
- **良好的可观测性**：详细的状态监控和进度跟踪
- **可扩展性**：支持大规模集群部署

通过实施该方案，可以实现分布式存储系统的高效、可靠的水平扩展，满足不断增长的数据存储需求。